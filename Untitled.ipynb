{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999, fetch_MTurk, fetch_RG65, fetch_RW\n",
    "from web.embeddings import fetch_GloVe, fetch_FastText, fetch_SG_GoogleNews, fetch_HDC, fetch_LexVec, fetch_HPCA\n",
    "from web.evaluate import evaluate_similarity\n",
    "import numpy as np\n",
    "from web.embedding import Embedding\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "from torch import optim\n",
    "from models.autoencoders import *\n",
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe, fetch_SG_GoogleNews\n",
    "from web.evaluate import get_vector_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(name):\n",
    "    root = \"C:/Users/jimon/Projects/word-embeddings-benchmarks/models/\"\n",
    "    with open(root + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dictionary = load_embeddings('embeddings/similarity_word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(embedding_path = 'embeddings/similarity_word_embeddings'):\n",
    "    embedding_dictionary = load_embeddings(embedding_path)\n",
    "    vectors = []\n",
    "    \n",
    "    for i, (name, words) in enumerate(embedding_dictionary.items()):\n",
    "        word_v = np.vstack(list(words.values()))    \n",
    "        vectors.append(word_v)\n",
    "        if i+1 == len(embedding_dictionary):\n",
    "            all_vectors = np.hstack(vectors)\n",
    "            torch_vectors = torch.from_numpy(all_vectors)\n",
    "            x = torch.utils.data.DataLoader(torch_vectors, batch_size=BATCH_SIZE, shuffle = False, num_workers= 6)\n",
    "            concat_vector = dict(zip(list(words), all_vectors))\n",
    "\n",
    "    # return word_vec dictionary and batched tensors \n",
    "    return (concat_vector, x)\n",
    "\n",
    "\n",
    "cv, v = get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv.values(), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_embedding_dictionary = load_embeddings('relational_embeddings/relational_ae_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.84822381e-02,  7.54341157e-03, -2.03369721e-03,\n",
       "        -2.69442750e-03,  8.47836956e-03, -1.50063569e-02,\n",
       "        -4.46629561e-02,  2.00204179e-03, -6.19076146e-03,\n",
       "        -9.01690945e-02,  1.94340618e-03,  1.35179004e-02,\n",
       "        -4.69364179e-03, -1.02345347e-02, -1.03988927e-02,\n",
       "         4.49555665e-02, -9.84645681e-04,  1.17171435e-02,\n",
       "        -8.60917382e-04, -8.64750298e-04,  2.81225424e-03,\n",
       "        -8.51988699e-03, -1.02605792e-02, -1.19336490e-02,\n",
       "         1.90852936e-02, -6.13537198e-03, -2.37188023e-02,\n",
       "         6.16427185e-03,  9.92451049e-03, -2.87601468e-03,\n",
       "         1.47487530e-02, -8.02740734e-03, -1.90701813e-03,\n",
       "         2.06096582e-02,  4.12944471e-03, -2.01463271e-02,\n",
       "        -4.00732383e-02, -1.34152221e-02, -5.02323136e-02,\n",
       "        -4.72784899e-02, -1.70123260e-02, -1.07829273e-02,\n",
       "        -2.48695314e-02,  1.32516557e-02, -3.23579647e-03,\n",
       "         1.00967819e-02, -4.77239070e-03, -1.50932921e-02,\n",
       "         1.96205033e-03, -1.06733879e-02, -4.51354682e-03,\n",
       "         1.05461739e-02, -1.60043743e-02,  4.59881127e-02,\n",
       "         1.47708375e-02, -4.18670446e-04,  1.20537234e-02,\n",
       "         1.46609228e-02,  1.02065364e-02,  3.04468255e-02,\n",
       "        -4.36381437e-03, -4.10823151e-02,  1.93385617e-03,\n",
       "         1.30927563e-02, -2.39735804e-02,  1.90804992e-02,\n",
       "         3.08556370e-02, -4.94631864e-02, -1.23136416e-02,\n",
       "         4.13388433e-03, -1.38605107e-02,  1.95902921e-02,\n",
       "         7.48458970e-03,  2.81371884e-02, -1.66133270e-02,\n",
       "         2.97838286e-03,  1.92141789e-03, -6.88848493e-04,\n",
       "         5.87868737e-03, -4.69321758e-02,  1.93167217e-02,\n",
       "         4.73737903e-03, -5.11254296e-02,  2.12851726e-03,\n",
       "        -1.88967667e-03,  4.54579899e-03, -1.93106607e-02,\n",
       "         1.13205109e-02, -2.82213348e-03, -3.98142915e-03,\n",
       "         5.72712487e-03,  3.93034937e-03, -6.88106287e-04,\n",
       "        -7.49323564e-03,  4.56401380e-03,  1.22717516e-02,\n",
       "        -3.39574441e-02, -1.43434471e-02,  3.18362541e-03,\n",
       "        -1.08558184e-03,  1.86752137e-02, -6.62155100e-04,\n",
       "         1.02019329e-02,  5.35612367e-03, -1.63466223e-02,\n",
       "         1.56554915e-02, -2.66987383e-02, -1.20276976e-02,\n",
       "        -5.62634692e-03, -3.34893689e-02,  1.73910731e-03,\n",
       "         4.28271480e-03,  1.67880449e-02, -8.51643831e-03,\n",
       "        -2.06892081e-02, -1.79285184e-02, -6.36366243e-03,\n",
       "        -8.12931452e-03,  1.32164555e-02, -1.24345496e-02,\n",
       "        -4.35799221e-03,  8.32526188e-04, -3.00956983e-02,\n",
       "         2.05165073e-02,  1.43684363e-02, -2.54823398e-02,\n",
       "         2.92520039e-03,  2.81502143e-03,  6.76245987e-02,\n",
       "        -1.50354151e-02,  2.49626674e-02, -4.55046725e-03,\n",
       "        -2.73556206e-02, -1.25071779e-03, -1.05014835e-02,\n",
       "        -5.62538025e-06, -5.23958029e-03, -3.43900695e-02,\n",
       "        -1.20684598e-02, -4.28135097e-02, -1.02185784e-02,\n",
       "         2.06477795e-04,  1.34835532e-02, -1.90763660e-02,\n",
       "        -5.49148209e-03, -8.39689048e-04, -1.34742418e-02,\n",
       "        -9.20507312e-03,  7.25933816e-03, -5.79955755e-03,\n",
       "         6.35089651e-02, -1.07344901e-02,  1.00649484e-02,\n",
       "         1.51165873e-02, -1.67597644e-02, -1.80782191e-03,\n",
       "        -6.86949044e-02,  9.61332396e-03,  2.30409466e-02,\n",
       "         1.00793317e-03, -2.08585728e-02, -1.99481822e-03,\n",
       "        -1.67796470e-03, -1.25571655e-03, -2.45558508e-02,\n",
       "        -3.98542220e-03, -8.11808743e-03,  3.42016737e-03,\n",
       "         1.48700606e-02, -4.37051756e-03, -2.72773579e-02,\n",
       "         5.47313783e-03,  1.63280673e-03, -9.34495311e-03,\n",
       "        -4.38060239e-02, -7.33819371e-03,  4.60007787e-03,\n",
       "         1.11844763e-01, -1.69193198e-03, -1.27337850e-03,\n",
       "        -2.87589297e-04, -2.14497894e-02,  2.61682505e-03,\n",
       "        -2.53015128e-03,  3.79358418e-02,  2.07048580e-02,\n",
       "         9.76695679e-03,  1.00996746e-02,  3.04762665e-02,\n",
       "         1.64760053e-02, -4.52441163e-03, -1.96205731e-02,\n",
       "         6.44752523e-03, -7.64788166e-02, -1.03301909e-02,\n",
       "         6.60812482e-03,  1.38487490e-02, -8.37669335e-03,\n",
       "        -1.61290802e-02,  2.71704290e-02, -1.66006889e-02,\n",
       "         5.15152188e-03,  3.45061324e-03, -6.32997369e-03,\n",
       "        -2.02520229e-02,  1.16361445e-02, -9.95171070e-03,\n",
       "        -4.22188733e-03, -2.34601349e-02,  1.34157054e-02,\n",
       "         6.91088894e-03, -8.60746193e-04, -8.06034170e-03,\n",
       "         2.61045760e-03, -4.53904979e-02,  6.76161260e-04,\n",
       "        -1.00242840e-02, -1.36963383e-03, -1.09357908e-02,\n",
       "         4.58441535e-03, -6.20557461e-03, -1.36584276e-03,\n",
       "        -9.33534058e-04, -1.46919629e-03,  6.05163202e-02,\n",
       "         7.69431004e-03,  9.12027154e-03,  1.04084760e-02,\n",
       "        -2.69790227e-03, -2.08318746e-03, -6.22946478e-04,\n",
       "        -6.07374310e-03, -2.73201289e-03, -1.40435342e-03,\n",
       "        -8.21130536e-03, -1.09788245e-02, -1.30082006e-02,\n",
       "        -3.00693023e-03, -2.13824734e-02, -1.43715944e-02,\n",
       "        -2.29359511e-02,  3.76040203e-04,  9.07382742e-03,\n",
       "         7.19006080e-03,  9.64828208e-03,  5.59671270e-03,\n",
       "        -1.28125318e-03,  5.47909364e-03, -1.07166171e-02,\n",
       "         7.18056178e-03,  9.75862145e-03, -1.06738787e-02,\n",
       "         1.84566267e-02, -1.65997911e-03,  2.31688190e-02,\n",
       "        -1.32868730e-03,  7.49858515e-03,  8.97241384e-03,\n",
       "         3.49864690e-03, -7.48065300e-03,  2.87242029e-02,\n",
       "        -7.86200911e-02, -1.28033664e-02,  6.38538855e-04,\n",
       "         6.56771809e-02, -1.23946657e-02,  4.44257036e-02,\n",
       "        -1.85896992e-03,  4.84594563e-03,  4.21858206e-02,\n",
       "        -3.10153677e-03,  1.14393421e-02, -9.29570757e-03,\n",
       "        -9.51959845e-03,  5.94069809e-03,  7.62514537e-03,\n",
       "        -8.86301603e-03, -1.48847774e-02, -1.25726042e-02,\n",
       "         1.93399992e-02, -6.61793677e-03,  3.50524485e-03,\n",
       "         3.27080430e-04, -3.67495231e-03, -2.09569223e-02,\n",
       "         1.30434474e-02, -4.03168835e-02, -3.25353779e-02,\n",
       "         6.30884711e-03, -5.00911055e-03, -5.12401573e-02,\n",
       "        -1.05671631e-03,  5.88907441e-03, -1.68910008e-02,\n",
       "        -1.25897713e-02,  3.36514749e-02, -7.43477046e-03,\n",
       "         8.72706715e-03,  4.02940772e-02, -3.48097691e-03]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(relational_embedding_dictionary['words'].keys())\n",
    "np.mean(relational_embedding_dictionary['glove'].data.numpy(), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(relational_embedding_dictionary['words'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.74675778e-02, -7.67301628e-03, -8.19895940e-04, -5.66484667e-02,\n",
       "       -3.24198380e-02, -3.16355005e-02, -5.85504659e-02,  1.20083205e-01,\n",
       "        4.49716747e-02, -3.83278751e-03,  9.63665321e-02,  3.33513394e-02,\n",
       "        7.07606822e-02, -1.35025546e-01, -7.38053918e-02,  7.72920549e-02,\n",
       "        1.12182871e-01, -4.25274409e-02,  1.15023464e-01,  4.94475216e-02,\n",
       "       -2.73620095e-02,  3.33505240e-03,  5.63782863e-02, -7.12720007e-02,\n",
       "       -2.88895536e-02, -1.48614779e-01, -3.07130758e-02, -3.36990878e-03,\n",
       "        6.94197044e-02, -4.29556221e-02, -1.46164596e-01,  1.01743199e-01,\n",
       "        7.34356567e-02, -7.18949810e-02,  1.28008034e-02, -1.78693384e-02,\n",
       "        8.23568925e-02, -6.82445467e-02,  4.26781476e-02, -1.38231456e-01,\n",
       "        4.32883110e-03,  1.11160828e-02,  5.29375225e-02,  9.00422111e-02,\n",
       "        6.16673045e-02, -3.58804725e-02, -7.64061464e-03, -7.39337057e-02,\n",
       "       -5.71737178e-02,  1.14618223e-02, -3.52486735e-04,  1.51619242e-04,\n",
       "       -2.33199038e-02, -1.12824984e-01, -1.11782262e-02,  9.65431333e-02,\n",
       "        2.63676550e-02,  1.06509611e-01, -3.52584594e-03, -7.62602407e-03,\n",
       "       -3.91803542e-03,  4.32606302e-02,  7.52168223e-02,  1.84859455e-01,\n",
       "       -9.74067822e-02,  1.10373087e-01,  7.38084838e-02, -1.30389065e-01,\n",
       "       -2.57118372e-03, -7.16797113e-02, -4.90394719e-02, -2.50776708e-02,\n",
       "       -1.08819626e-01, -5.35327718e-02,  1.41761392e-01, -8.31737965e-02,\n",
       "       -1.15822531e-01,  8.33492726e-03,  1.09187618e-01,  5.54540344e-02,\n",
       "        4.21152487e-02,  4.37849015e-02, -2.48070844e-02,  2.76234634e-02,\n",
       "        4.44531674e-03,  9.29996967e-02, -1.05455928e-01,  7.53247589e-02,\n",
       "        1.17044955e-01, -8.84654894e-02, -2.92006675e-02, -1.52408546e-02,\n",
       "       -1.22082815e-01,  4.24207114e-02,  1.08195789e-01, -9.49031636e-02,\n",
       "       -2.82679237e-02,  8.04736186e-03,  6.63418695e-02,  1.01976935e-02,\n",
       "        5.28561771e-02,  4.67510056e-03,  1.83440726e-02, -4.67050672e-02,\n",
       "        5.34519777e-02,  1.74733996e-01, -7.14873150e-02,  1.15922421e-01,\n",
       "        7.83592612e-02, -7.33795688e-02,  7.32718268e-03, -6.19522780e-02,\n",
       "        1.90187301e-02,  2.06172727e-02, -2.93740835e-02,  2.12202631e-02,\n",
       "       -2.42588315e-02, -4.25369516e-02,  2.08335172e-04,  7.08903000e-02,\n",
       "       -8.30748007e-02,  4.93199192e-03, -2.02277452e-02,  3.34268436e-02,\n",
       "        1.01562344e-01, -2.38505248e-02,  4.38978374e-02, -1.55486548e-02,\n",
       "        3.54225673e-02,  4.93012369e-02,  8.34915042e-02, -8.84437561e-02,\n",
       "       -1.56864412e-02, -7.15901703e-02, -7.14614093e-02, -3.69327478e-02,\n",
       "       -5.76214343e-02, -1.15061946e-01,  8.42548087e-02, -5.92768639e-02,\n",
       "       -9.92503483e-03,  4.71758135e-02,  5.86099103e-02, -1.44269630e-01,\n",
       "        1.71219325e-03,  1.16346814e-01, -2.42616460e-02, -5.70327528e-02,\n",
       "        2.42356341e-02, -3.08326166e-02,  5.50839938e-02, -1.49230193e-02,\n",
       "        1.66443978e-02,  1.98376272e-02,  5.25906682e-02, -7.94084445e-02,\n",
       "       -1.01257369e-01,  4.88239899e-02, -4.15077209e-02,  2.62005739e-02,\n",
       "       -2.61750389e-02,  1.99378077e-02, -6.07879236e-02, -5.50743416e-02,\n",
       "        1.12279691e-02,  1.19273430e-02, -1.18504688e-01,  3.61043401e-02,\n",
       "       -1.10094368e-01, -5.79686053e-02, -1.16638746e-02, -7.11729564e-03,\n",
       "        5.79774864e-02, -4.89875525e-02, -6.31613145e-03,  1.56981051e-02,\n",
       "        1.04075372e-02,  2.11396962e-01, -6.10051490e-02, -2.91139819e-02,\n",
       "        7.39264563e-02, -3.89772505e-02,  6.42146617e-02, -2.05218326e-02,\n",
       "        7.88580254e-02,  9.33557004e-02,  9.14782751e-03, -2.73950249e-02,\n",
       "       -2.91876327e-02, -1.02823563e-01, -1.14143729e-01,  5.74239762e-03,\n",
       "        6.49380609e-02, -1.59547195e-01,  2.06173882e-02,  9.57049355e-02,\n",
       "       -1.30023668e-02, -5.60332797e-02,  5.68184257e-02,  6.56121224e-02,\n",
       "        1.21791810e-01,  1.85659584e-02,  1.06882267e-01,  7.41055682e-02,\n",
       "        1.03437513e-01, -5.45161515e-02, -1.03563987e-01,  1.03061143e-02,\n",
       "       -3.08841802e-02, -7.68419877e-02, -6.41039088e-02, -5.66180702e-03,\n",
       "       -4.17091884e-02, -7.76508823e-02, -1.41273052e-01, -5.97175173e-02,\n",
       "       -2.29365490e-02, -1.57702509e-02,  3.89757492e-02,  5.01072519e-02,\n",
       "       -1.28047774e-02, -2.24129390e-03,  1.23555362e-01, -6.25825115e-03,\n",
       "        1.57488078e-01, -2.63352669e-03,  4.75573279e-02, -1.63662434e-02,\n",
       "       -2.93224081e-02,  6.98364004e-02,  1.88580696e-02, -7.61093497e-02,\n",
       "       -4.81238887e-02,  1.03218958e-01,  9.03658643e-02, -5.81151769e-02,\n",
       "        1.05417715e-02, -3.01617645e-02, -4.29837368e-02,  1.15428910e-01,\n",
       "       -4.65118214e-02, -3.99214327e-02, -1.26130003e-02,  2.09828583e-03,\n",
       "       -1.81368683e-02,  3.58740017e-02, -1.50767639e-01, -1.22002542e-01,\n",
       "       -1.02895290e-01, -9.63964984e-02,  8.16084519e-02, -2.06282027e-02,\n",
       "       -6.29354864e-02,  1.51003273e-02, -2.55296268e-02, -1.39234379e-01,\n",
       "       -5.47482856e-02, -1.09957747e-01, -4.19615954e-02,  1.04603572e-02,\n",
       "        1.52068269e-02, -1.79204851e-01, -2.95334831e-02,  1.00625984e-01,\n",
       "        7.59037808e-02, -1.50754098e-02,  1.94953345e-02, -1.89144835e-02,\n",
       "        3.32007334e-02,  1.09542713e-01, -2.86573712e-02,  8.21765959e-02,\n",
       "        1.36453556e-02,  1.23431720e-01, -2.07765717e-02, -6.00841790e-02,\n",
       "        3.32946554e-02, -1.10598065e-01,  6.99265599e-02,  1.47881182e-02,\n",
       "       -2.73575447e-02, -8.93649179e-03,  3.54007222e-02, -1.03187352e-01,\n",
       "        2.48722229e-02,  1.57714169e-02,  7.19193444e-02, -1.45610643e-03,\n",
       "       -2.11792458e-02, -1.20047636e-01, -1.68977678e-02, -3.65714766e-02,\n",
       "       -1.79779239e-03, -1.27258077e-01,  9.89917815e-02, -3.76241095e-02,\n",
       "       -8.87458101e-02,  4.26925980e-02,  1.01990653e-02,  7.60393590e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_embedding_dictionary['glove'][0].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(relational_embedding_dictionary['words'].keys()).index(\"nurse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "relational_embedding_loss = load_embeddings('relational_ae_embeddings_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HPCA': [tensor(1.00000e-03 *\n",
       "         4.6448), tensor(1.00000e-03 *\n",
       "         4.3246), tensor(1.00000e-03 *\n",
       "         3.9670), tensor(1.00000e-03 *\n",
       "         3.5797), tensor(1.00000e-03 *\n",
       "         3.3039), tensor(1.00000e-03 *\n",
       "         3.1700), tensor(1.00000e-03 *\n",
       "         3.1661), tensor(1.00000e-03 *\n",
       "         3.0845), tensor(1.00000e-03 *\n",
       "         3.1975), tensor(1.00000e-03 *\n",
       "         3.0732), tensor(1.00000e-03 *\n",
       "         2.8994), tensor(1.00000e-03 *\n",
       "         2.9719), tensor(1.00000e-03 *\n",
       "         2.9309), tensor(1.00000e-03 *\n",
       "         2.8856), tensor(1.00000e-03 *\n",
       "         2.7500), tensor(1.00000e-03 *\n",
       "         2.7041), tensor(1.00000e-03 *\n",
       "         2.7897), tensor(1.00000e-03 *\n",
       "         2.4972), tensor(1.00000e-03 *\n",
       "         2.5747), tensor(1.00000e-03 *\n",
       "         2.6068), tensor(1.00000e-03 *\n",
       "         2.6301), tensor(1.00000e-03 *\n",
       "         2.5019), tensor(1.00000e-03 *\n",
       "         2.3561), tensor(1.00000e-03 *\n",
       "         2.1778), tensor(1.00000e-03 *\n",
       "         2.4752), tensor(1.00000e-03 *\n",
       "         2.2602), tensor(1.00000e-03 *\n",
       "         2.1941), tensor(1.00000e-03 *\n",
       "         2.1224), tensor(1.00000e-03 *\n",
       "         2.2888), tensor(1.00000e-03 *\n",
       "         2.1724), tensor(1.00000e-03 *\n",
       "         2.1663), tensor(1.00000e-03 *\n",
       "         2.3103), tensor(1.00000e-03 *\n",
       "         2.2177), tensor(1.00000e-03 *\n",
       "         2.1243), tensor(1.00000e-03 *\n",
       "         2.0951), tensor(1.00000e-03 *\n",
       "         2.2058), tensor(1.00000e-03 *\n",
       "         2.0979), tensor(1.00000e-03 *\n",
       "         2.0782), tensor(1.00000e-03 *\n",
       "         2.1362), tensor(1.00000e-03 *\n",
       "         1.7817), tensor(1.00000e-03 *\n",
       "         2.2699), tensor(1.00000e-03 *\n",
       "         1.9094), tensor(1.00000e-03 *\n",
       "         2.2108), tensor(1.00000e-03 *\n",
       "         1.9771), tensor(1.00000e-03 *\n",
       "         2.0907), tensor(1.00000e-03 *\n",
       "         2.0307), tensor(1.00000e-03 *\n",
       "         1.9423), tensor(1.00000e-03 *\n",
       "         2.0041), tensor(1.00000e-03 *\n",
       "         1.7958), tensor(1.00000e-03 *\n",
       "         1.9137), tensor(1.00000e-03 *\n",
       "         1.8124), tensor(1.00000e-03 *\n",
       "         1.8619), tensor(1.00000e-03 *\n",
       "         1.7535), tensor(1.00000e-03 *\n",
       "         1.9313), tensor(1.00000e-03 *\n",
       "         1.8448), tensor(1.00000e-03 *\n",
       "         1.9242), tensor(1.00000e-03 *\n",
       "         1.7277), tensor(1.00000e-03 *\n",
       "         1.7839), tensor(1.00000e-03 *\n",
       "         1.6725), tensor(1.00000e-03 *\n",
       "         1.5453), tensor(1.00000e-03 *\n",
       "         1.6362), tensor(1.00000e-03 *\n",
       "         1.7460), tensor(1.00000e-03 *\n",
       "         1.6696), tensor(1.00000e-03 *\n",
       "         1.6780), tensor(1.00000e-03 *\n",
       "         1.5884), tensor(1.00000e-03 *\n",
       "         1.6896), tensor(1.00000e-03 *\n",
       "         1.6425), tensor(1.00000e-03 *\n",
       "         1.4898), tensor(1.00000e-03 *\n",
       "         1.5596), tensor(1.00000e-03 *\n",
       "         1.8298), tensor(1.00000e-03 *\n",
       "         1.5399), tensor(1.00000e-03 *\n",
       "         1.6861), tensor(1.00000e-03 *\n",
       "         1.5016), tensor(1.00000e-03 *\n",
       "         1.5893), tensor(1.00000e-03 *\n",
       "         1.4986), tensor(1.00000e-03 *\n",
       "         1.6078), tensor(1.00000e-03 *\n",
       "         1.5173), tensor(1.00000e-03 *\n",
       "         1.5392), tensor(1.00000e-03 *\n",
       "         1.3905), tensor(1.00000e-03 *\n",
       "         1.5895), tensor(1.00000e-03 *\n",
       "         1.5671), tensor(1.00000e-03 *\n",
       "         1.3457), tensor(1.00000e-03 *\n",
       "         1.4395), tensor(1.00000e-03 *\n",
       "         1.4558), tensor(1.00000e-03 *\n",
       "         1.5566), tensor(1.00000e-03 *\n",
       "         1.3408), tensor(1.00000e-03 *\n",
       "         1.6120), tensor(1.00000e-03 *\n",
       "         1.5262), tensor(1.00000e-03 *\n",
       "         1.4943), tensor(1.00000e-03 *\n",
       "         1.4083), tensor(1.00000e-03 *\n",
       "         1.4810), tensor(1.00000e-03 *\n",
       "         1.4845), tensor(1.00000e-03 *\n",
       "         1.4166), tensor(1.00000e-03 *\n",
       "         1.4206), tensor(1.00000e-03 *\n",
       "         1.3710), tensor(1.00000e-03 *\n",
       "         1.3325), tensor(1.00000e-03 *\n",
       "         1.4502), tensor(1.00000e-03 *\n",
       "         1.2759), tensor(1.00000e-03 *\n",
       "         1.3661), tensor(1.00000e-03 *\n",
       "         1.1806), tensor(1.00000e-03 *\n",
       "         1.3483), tensor(1.00000e-03 *\n",
       "         1.3521), tensor(1.00000e-03 *\n",
       "         1.2946), tensor(1.00000e-03 *\n",
       "         1.1480), tensor(1.00000e-03 *\n",
       "         1.3449), tensor(1.00000e-03 *\n",
       "         1.3920), tensor(1.00000e-03 *\n",
       "         1.3522), tensor(1.00000e-03 *\n",
       "         1.3639), tensor(1.00000e-03 *\n",
       "         1.3151), tensor(1.00000e-03 *\n",
       "         1.0744), tensor(1.00000e-03 *\n",
       "         1.3445), tensor(1.00000e-03 *\n",
       "         1.3713), tensor(1.00000e-03 *\n",
       "         1.4145), tensor(1.00000e-03 *\n",
       "         1.1140), tensor(1.00000e-03 *\n",
       "         1.1998), tensor(1.00000e-03 *\n",
       "         1.2164), tensor(1.00000e-03 *\n",
       "         1.1100), tensor(1.00000e-03 *\n",
       "         1.2094), tensor(1.00000e-03 *\n",
       "         1.3245), tensor(1.00000e-03 *\n",
       "         1.1950), tensor(1.00000e-03 *\n",
       "         1.0680), tensor(1.00000e-03 *\n",
       "         1.1758), tensor(1.00000e-03 *\n",
       "         1.2588), tensor(1.00000e-03 *\n",
       "         1.2684), tensor(1.00000e-03 *\n",
       "         1.1234), tensor(1.00000e-03 *\n",
       "         1.1612), tensor(1.00000e-03 *\n",
       "         1.1803), tensor(1.00000e-03 *\n",
       "         1.3130), tensor(1.00000e-03 *\n",
       "         1.1990), tensor(1.00000e-03 *\n",
       "         1.0663), tensor(1.00000e-03 *\n",
       "         1.0727), tensor(1.00000e-03 *\n",
       "         1.0593), tensor(1.00000e-03 *\n",
       "         1.1948), tensor(1.00000e-03 *\n",
       "         1.0811), tensor(1.00000e-03 *\n",
       "         1.0989), tensor(1.00000e-03 *\n",
       "         1.1646), tensor(1.00000e-03 *\n",
       "         1.0460), tensor(1.00000e-04 *\n",
       "         9.9273), tensor(1.00000e-03 *\n",
       "         1.1473), tensor(1.00000e-03 *\n",
       "         1.1325), tensor(1.00000e-03 *\n",
       "         1.0920), tensor(1.00000e-03 *\n",
       "         1.0255), tensor(1.00000e-03 *\n",
       "         1.0581), tensor(1.00000e-03 *\n",
       "         1.0928), tensor(1.00000e-03 *\n",
       "         1.0155), tensor(1.00000e-03 *\n",
       "         1.0689), tensor(1.00000e-03 *\n",
       "         1.0006), tensor(1.00000e-03 *\n",
       "         1.0563), tensor(1.00000e-04 *\n",
       "         9.9778), tensor(1.00000e-03 *\n",
       "         1.1074), tensor(1.00000e-03 *\n",
       "         1.1016)], 'fasttext': [tensor(1.00000e-03 *\n",
       "         4.6530), tensor(1.00000e-03 *\n",
       "         4.4293), tensor(1.00000e-03 *\n",
       "         4.1875), tensor(1.00000e-03 *\n",
       "         3.7494), tensor(1.00000e-03 *\n",
       "         3.6319), tensor(1.00000e-03 *\n",
       "         3.4295), tensor(1.00000e-03 *\n",
       "         3.3489), tensor(1.00000e-03 *\n",
       "         3.2079), tensor(1.00000e-03 *\n",
       "         3.3183), tensor(1.00000e-03 *\n",
       "         3.0808), tensor(1.00000e-03 *\n",
       "         3.0084), tensor(1.00000e-03 *\n",
       "         3.1367), tensor(1.00000e-03 *\n",
       "         3.0838), tensor(1.00000e-03 *\n",
       "         3.0554), tensor(1.00000e-03 *\n",
       "         2.9412), tensor(1.00000e-03 *\n",
       "         2.9205), tensor(1.00000e-03 *\n",
       "         2.9323), tensor(1.00000e-03 *\n",
       "         2.6708), tensor(1.00000e-03 *\n",
       "         2.6609), tensor(1.00000e-03 *\n",
       "         2.7401), tensor(1.00000e-03 *\n",
       "         2.7610), tensor(1.00000e-03 *\n",
       "         2.6978), tensor(1.00000e-03 *\n",
       "         2.5401), tensor(1.00000e-03 *\n",
       "         2.3566), tensor(1.00000e-03 *\n",
       "         2.6694), tensor(1.00000e-03 *\n",
       "         2.4276), tensor(1.00000e-03 *\n",
       "         2.3725), tensor(1.00000e-03 *\n",
       "         2.3279), tensor(1.00000e-03 *\n",
       "         2.4698), tensor(1.00000e-03 *\n",
       "         2.3333), tensor(1.00000e-03 *\n",
       "         2.3335), tensor(1.00000e-03 *\n",
       "         2.5182), tensor(1.00000e-03 *\n",
       "         2.4534), tensor(1.00000e-03 *\n",
       "         2.2824), tensor(1.00000e-03 *\n",
       "         2.2238), tensor(1.00000e-03 *\n",
       "         2.3565), tensor(1.00000e-03 *\n",
       "         2.2548), tensor(1.00000e-03 *\n",
       "         2.1814), tensor(1.00000e-03 *\n",
       "         2.3589), tensor(1.00000e-03 *\n",
       "         1.9865), tensor(1.00000e-03 *\n",
       "         2.4321), tensor(1.00000e-03 *\n",
       "         2.1259), tensor(1.00000e-03 *\n",
       "         2.3621), tensor(1.00000e-03 *\n",
       "         2.1736), tensor(1.00000e-03 *\n",
       "         2.2812), tensor(1.00000e-03 *\n",
       "         2.2199), tensor(1.00000e-03 *\n",
       "         2.1399), tensor(1.00000e-03 *\n",
       "         2.1752), tensor(1.00000e-03 *\n",
       "         2.0222), tensor(1.00000e-03 *\n",
       "         2.0672), tensor(1.00000e-03 *\n",
       "         2.0100), tensor(1.00000e-03 *\n",
       "         2.0217), tensor(1.00000e-03 *\n",
       "         1.9180), tensor(1.00000e-03 *\n",
       "         2.1121), tensor(1.00000e-03 *\n",
       "         2.0734), tensor(1.00000e-03 *\n",
       "         2.0759), tensor(1.00000e-03 *\n",
       "         1.9343), tensor(1.00000e-03 *\n",
       "         1.9610), tensor(1.00000e-03 *\n",
       "         1.8731), tensor(1.00000e-03 *\n",
       "         1.7304), tensor(1.00000e-03 *\n",
       "         1.8172), tensor(1.00000e-03 *\n",
       "         1.8999), tensor(1.00000e-03 *\n",
       "         1.8891), tensor(1.00000e-03 *\n",
       "         1.8749), tensor(1.00000e-03 *\n",
       "         1.8022), tensor(1.00000e-03 *\n",
       "         1.8629), tensor(1.00000e-03 *\n",
       "         1.8203), tensor(1.00000e-03 *\n",
       "         1.6565), tensor(1.00000e-03 *\n",
       "         1.7293), tensor(1.00000e-03 *\n",
       "         2.0022), tensor(1.00000e-03 *\n",
       "         1.7503), tensor(1.00000e-03 *\n",
       "         1.8827), tensor(1.00000e-03 *\n",
       "         1.7689), tensor(1.00000e-03 *\n",
       "         1.7936), tensor(1.00000e-03 *\n",
       "         1.6862), tensor(1.00000e-03 *\n",
       "         1.8095), tensor(1.00000e-03 *\n",
       "         1.7469), tensor(1.00000e-03 *\n",
       "         1.7195), tensor(1.00000e-03 *\n",
       "         1.5468), tensor(1.00000e-03 *\n",
       "         1.7497), tensor(1.00000e-03 *\n",
       "         1.8039), tensor(1.00000e-03 *\n",
       "         1.4839), tensor(1.00000e-03 *\n",
       "         1.6581), tensor(1.00000e-03 *\n",
       "         1.7046), tensor(1.00000e-03 *\n",
       "         1.7343), tensor(1.00000e-03 *\n",
       "         1.5195), tensor(1.00000e-03 *\n",
       "         1.7852), tensor(1.00000e-03 *\n",
       "         1.7126), tensor(1.00000e-03 *\n",
       "         1.6521), tensor(1.00000e-03 *\n",
       "         1.6356), tensor(1.00000e-03 *\n",
       "         1.7021), tensor(1.00000e-03 *\n",
       "         1.6747), tensor(1.00000e-03 *\n",
       "         1.5946), tensor(1.00000e-03 *\n",
       "         1.6552), tensor(1.00000e-03 *\n",
       "         1.5870), tensor(1.00000e-03 *\n",
       "         1.5536), tensor(1.00000e-03 *\n",
       "         1.6319), tensor(1.00000e-03 *\n",
       "         1.5067), tensor(1.00000e-03 *\n",
       "         1.5558), tensor(1.00000e-03 *\n",
       "         1.3790), tensor(1.00000e-03 *\n",
       "         1.5497), tensor(1.00000e-03 *\n",
       "         1.5140), tensor(1.00000e-03 *\n",
       "         1.5505), tensor(1.00000e-03 *\n",
       "         1.3548), tensor(1.00000e-03 *\n",
       "         1.5177), tensor(1.00000e-03 *\n",
       "         1.5795), tensor(1.00000e-03 *\n",
       "         1.5770), tensor(1.00000e-03 *\n",
       "         1.4916), tensor(1.00000e-03 *\n",
       "         1.5230), tensor(1.00000e-03 *\n",
       "         1.3082), tensor(1.00000e-03 *\n",
       "         1.4827), tensor(1.00000e-03 *\n",
       "         1.5970), tensor(1.00000e-03 *\n",
       "         1.6405), tensor(1.00000e-03 *\n",
       "         1.2557), tensor(1.00000e-03 *\n",
       "         1.3959), tensor(1.00000e-03 *\n",
       "         1.4023), tensor(1.00000e-03 *\n",
       "         1.3184), tensor(1.00000e-03 *\n",
       "         1.4632), tensor(1.00000e-03 *\n",
       "         1.5464), tensor(1.00000e-03 *\n",
       "         1.4156), tensor(1.00000e-03 *\n",
       "         1.2660), tensor(1.00000e-03 *\n",
       "         1.3457), tensor(1.00000e-03 *\n",
       "         1.4520), tensor(1.00000e-03 *\n",
       "         1.4797), tensor(1.00000e-03 *\n",
       "         1.3495), tensor(1.00000e-03 *\n",
       "         1.3776), tensor(1.00000e-03 *\n",
       "         1.4361), tensor(1.00000e-03 *\n",
       "         1.5200), tensor(1.00000e-03 *\n",
       "         1.3600), tensor(1.00000e-03 *\n",
       "         1.2492), tensor(1.00000e-03 *\n",
       "         1.2676), tensor(1.00000e-03 *\n",
       "         1.2410), tensor(1.00000e-03 *\n",
       "         1.3934), tensor(1.00000e-03 *\n",
       "         1.2924), tensor(1.00000e-03 *\n",
       "         1.3120), tensor(1.00000e-03 *\n",
       "         1.3991), tensor(1.00000e-03 *\n",
       "         1.2289), tensor(1.00000e-03 *\n",
       "         1.1911), tensor(1.00000e-03 *\n",
       "         1.3311), tensor(1.00000e-03 *\n",
       "         1.3164), tensor(1.00000e-03 *\n",
       "         1.3008), tensor(1.00000e-03 *\n",
       "         1.2229), tensor(1.00000e-03 *\n",
       "         1.2653), tensor(1.00000e-03 *\n",
       "         1.2791), tensor(1.00000e-03 *\n",
       "         1.2073), tensor(1.00000e-03 *\n",
       "         1.2379), tensor(1.00000e-03 *\n",
       "         1.1916), tensor(1.00000e-03 *\n",
       "         1.2185), tensor(1.00000e-03 *\n",
       "         1.2055), tensor(1.00000e-03 *\n",
       "         1.3242), tensor(1.00000e-03 *\n",
       "         1.3186)], 'glove': [tensor(1.00000e-03 *\n",
       "         4.8592), tensor(1.00000e-03 *\n",
       "         4.2887), tensor(1.00000e-03 *\n",
       "         3.9141), tensor(1.00000e-03 *\n",
       "         3.4482), tensor(1.00000e-03 *\n",
       "         3.4658), tensor(1.00000e-03 *\n",
       "         3.2345), tensor(1.00000e-03 *\n",
       "         3.1316), tensor(1.00000e-03 *\n",
       "         2.8536), tensor(1.00000e-03 *\n",
       "         2.9337), tensor(1.00000e-03 *\n",
       "         2.8262), tensor(1.00000e-03 *\n",
       "         2.7739), tensor(1.00000e-03 *\n",
       "         2.7444), tensor(1.00000e-03 *\n",
       "         2.7723), tensor(1.00000e-03 *\n",
       "         2.7753), tensor(1.00000e-03 *\n",
       "         2.5324), tensor(1.00000e-03 *\n",
       "         2.4726), tensor(1.00000e-03 *\n",
       "         2.5873), tensor(1.00000e-03 *\n",
       "         2.4497), tensor(1.00000e-03 *\n",
       "         2.5149), tensor(1.00000e-03 *\n",
       "         2.5332), tensor(1.00000e-03 *\n",
       "         2.5652), tensor(1.00000e-03 *\n",
       "         2.4678), tensor(1.00000e-03 *\n",
       "         2.3347), tensor(1.00000e-03 *\n",
       "         2.2716), tensor(1.00000e-03 *\n",
       "         2.3561), tensor(1.00000e-03 *\n",
       "         2.3243), tensor(1.00000e-03 *\n",
       "         2.3139), tensor(1.00000e-03 *\n",
       "         2.2196), tensor(1.00000e-03 *\n",
       "         2.2479), tensor(1.00000e-03 *\n",
       "         2.3800), tensor(1.00000e-03 *\n",
       "         2.1506), tensor(1.00000e-03 *\n",
       "         2.2154), tensor(1.00000e-03 *\n",
       "         2.1027), tensor(1.00000e-03 *\n",
       "         2.0280), tensor(1.00000e-03 *\n",
       "         2.1439), tensor(1.00000e-03 *\n",
       "         2.0493), tensor(1.00000e-03 *\n",
       "         2.0434), tensor(1.00000e-03 *\n",
       "         2.0641), tensor(1.00000e-03 *\n",
       "         2.0906), tensor(1.00000e-03 *\n",
       "         1.9043), tensor(1.00000e-03 *\n",
       "         2.0688), tensor(1.00000e-03 *\n",
       "         2.0139), tensor(1.00000e-03 *\n",
       "         2.0148), tensor(1.00000e-03 *\n",
       "         2.0203), tensor(1.00000e-03 *\n",
       "         1.9493), tensor(1.00000e-03 *\n",
       "         2.0283), tensor(1.00000e-03 *\n",
       "         1.9983), tensor(1.00000e-03 *\n",
       "         2.0015), tensor(1.00000e-03 *\n",
       "         1.8173), tensor(1.00000e-03 *\n",
       "         1.8545), tensor(1.00000e-03 *\n",
       "         1.8382), tensor(1.00000e-03 *\n",
       "         1.6857), tensor(1.00000e-03 *\n",
       "         1.8265), tensor(1.00000e-03 *\n",
       "         1.8477), tensor(1.00000e-03 *\n",
       "         1.8741), tensor(1.00000e-03 *\n",
       "         1.8009), tensor(1.00000e-03 *\n",
       "         1.7803), tensor(1.00000e-03 *\n",
       "         1.7491), tensor(1.00000e-03 *\n",
       "         1.7066), tensor(1.00000e-03 *\n",
       "         1.7759), tensor(1.00000e-03 *\n",
       "         1.7602), tensor(1.00000e-03 *\n",
       "         1.7776), tensor(1.00000e-03 *\n",
       "         1.8556), tensor(1.00000e-03 *\n",
       "         1.7384), tensor(1.00000e-03 *\n",
       "         1.7481), tensor(1.00000e-03 *\n",
       "         1.5814), tensor(1.00000e-03 *\n",
       "         1.6298), tensor(1.00000e-03 *\n",
       "         1.6311), tensor(1.00000e-03 *\n",
       "         1.5888), tensor(1.00000e-03 *\n",
       "         1.7741), tensor(1.00000e-03 *\n",
       "         1.6421), tensor(1.00000e-03 *\n",
       "         1.6238), tensor(1.00000e-03 *\n",
       "         1.5836), tensor(1.00000e-03 *\n",
       "         1.5980), tensor(1.00000e-03 *\n",
       "         1.6326), tensor(1.00000e-03 *\n",
       "         1.6007), tensor(1.00000e-03 *\n",
       "         1.6445), tensor(1.00000e-03 *\n",
       "         1.5833), tensor(1.00000e-03 *\n",
       "         1.6332), tensor(1.00000e-03 *\n",
       "         1.5401), tensor(1.00000e-03 *\n",
       "         1.6092), tensor(1.00000e-03 *\n",
       "         1.4579), tensor(1.00000e-03 *\n",
       "         1.6393), tensor(1.00000e-03 *\n",
       "         1.5193), tensor(1.00000e-03 *\n",
       "         1.5805), tensor(1.00000e-03 *\n",
       "         1.5217), tensor(1.00000e-03 *\n",
       "         1.5578), tensor(1.00000e-03 *\n",
       "         1.4472), tensor(1.00000e-03 *\n",
       "         1.4454), tensor(1.00000e-03 *\n",
       "         1.4400), tensor(1.00000e-03 *\n",
       "         1.5619), tensor(1.00000e-03 *\n",
       "         1.3895), tensor(1.00000e-03 *\n",
       "         1.4889), tensor(1.00000e-03 *\n",
       "         1.4671), tensor(1.00000e-03 *\n",
       "         1.3309), tensor(1.00000e-03 *\n",
       "         1.4223), tensor(1.00000e-03 *\n",
       "         1.4444), tensor(1.00000e-03 *\n",
       "         1.4387), tensor(1.00000e-03 *\n",
       "         1.4057), tensor(1.00000e-03 *\n",
       "         1.3594), tensor(1.00000e-03 *\n",
       "         1.3946), tensor(1.00000e-03 *\n",
       "         1.3610), tensor(1.00000e-03 *\n",
       "         1.4077), tensor(1.00000e-03 *\n",
       "         1.3168), tensor(1.00000e-03 *\n",
       "         1.4316), tensor(1.00000e-03 *\n",
       "         1.4985), tensor(1.00000e-03 *\n",
       "         1.3465), tensor(1.00000e-03 *\n",
       "         1.3255), tensor(1.00000e-03 *\n",
       "         1.4130), tensor(1.00000e-03 *\n",
       "         1.2994), tensor(1.00000e-03 *\n",
       "         1.4165), tensor(1.00000e-03 *\n",
       "         1.3490), tensor(1.00000e-03 *\n",
       "         1.3415), tensor(1.00000e-03 *\n",
       "         1.2551), tensor(1.00000e-03 *\n",
       "         1.3415), tensor(1.00000e-03 *\n",
       "         1.3762), tensor(1.00000e-03 *\n",
       "         1.2559), tensor(1.00000e-03 *\n",
       "         1.3158), tensor(1.00000e-03 *\n",
       "         1.4348), tensor(1.00000e-03 *\n",
       "         1.3285), tensor(1.00000e-03 *\n",
       "         1.3007), tensor(1.00000e-03 *\n",
       "         1.2928), tensor(1.00000e-03 *\n",
       "         1.3862), tensor(1.00000e-03 *\n",
       "         1.2558), tensor(1.00000e-03 *\n",
       "         1.3469), tensor(1.00000e-03 *\n",
       "         1.2528), tensor(1.00000e-03 *\n",
       "         1.3276), tensor(1.00000e-03 *\n",
       "         1.2636), tensor(1.00000e-03 *\n",
       "         1.2620), tensor(1.00000e-03 *\n",
       "         1.1788), tensor(1.00000e-03 *\n",
       "         1.2876), tensor(1.00000e-03 *\n",
       "         1.3375), tensor(1.00000e-03 *\n",
       "         1.3605), tensor(1.00000e-03 *\n",
       "         1.2681), tensor(1.00000e-03 *\n",
       "         1.2716), tensor(1.00000e-03 *\n",
       "         1.1237), tensor(1.00000e-03 *\n",
       "         1.2211), tensor(1.00000e-03 *\n",
       "         1.2529), tensor(1.00000e-03 *\n",
       "         1.1328), tensor(1.00000e-03 *\n",
       "         1.1978), tensor(1.00000e-03 *\n",
       "         1.2037), tensor(1.00000e-03 *\n",
       "         1.2220), tensor(1.00000e-03 *\n",
       "         1.2102), tensor(1.00000e-03 *\n",
       "         1.1899), tensor(1.00000e-03 *\n",
       "         1.2192), tensor(1.00000e-03 *\n",
       "         1.0716), tensor(1.00000e-03 *\n",
       "         1.1984), tensor(1.00000e-03 *\n",
       "         1.2027), tensor(1.00000e-03 *\n",
       "         1.1528), tensor(1.00000e-03 *\n",
       "         1.2286), tensor(1.00000e-03 *\n",
       "         1.1095)], 'hdc': [tensor(1.00000e-03 *\n",
       "         4.5975), tensor(1.00000e-03 *\n",
       "         4.4655), tensor(1.00000e-03 *\n",
       "         4.1960), tensor(1.00000e-03 *\n",
       "         3.7634), tensor(1.00000e-03 *\n",
       "         3.6410), tensor(1.00000e-03 *\n",
       "         3.4087), tensor(1.00000e-03 *\n",
       "         3.3452), tensor(1.00000e-03 *\n",
       "         3.2556), tensor(1.00000e-03 *\n",
       "         3.2806), tensor(1.00000e-03 *\n",
       "         3.1099), tensor(1.00000e-03 *\n",
       "         3.0449), tensor(1.00000e-03 *\n",
       "         3.1133), tensor(1.00000e-03 *\n",
       "         3.0707), tensor(1.00000e-03 *\n",
       "         3.0966), tensor(1.00000e-03 *\n",
       "         2.9051), tensor(1.00000e-03 *\n",
       "         2.8848), tensor(1.00000e-03 *\n",
       "         2.9459), tensor(1.00000e-03 *\n",
       "         2.6369), tensor(1.00000e-03 *\n",
       "         2.6918), tensor(1.00000e-03 *\n",
       "         2.7468), tensor(1.00000e-03 *\n",
       "         2.7628), tensor(1.00000e-03 *\n",
       "         2.6765), tensor(1.00000e-03 *\n",
       "         2.5477), tensor(1.00000e-03 *\n",
       "         2.3612), tensor(1.00000e-03 *\n",
       "         2.6686), tensor(1.00000e-03 *\n",
       "         2.4525), tensor(1.00000e-03 *\n",
       "         2.3524), tensor(1.00000e-03 *\n",
       "         2.3125), tensor(1.00000e-03 *\n",
       "         2.4776), tensor(1.00000e-03 *\n",
       "         2.3332), tensor(1.00000e-03 *\n",
       "         2.3542), tensor(1.00000e-03 *\n",
       "         2.4829), tensor(1.00000e-03 *\n",
       "         2.4371), tensor(1.00000e-03 *\n",
       "         2.2919), tensor(1.00000e-03 *\n",
       "         2.2470), tensor(1.00000e-03 *\n",
       "         2.3301), tensor(1.00000e-03 *\n",
       "         2.2633), tensor(1.00000e-03 *\n",
       "         2.2170), tensor(1.00000e-03 *\n",
       "         2.3585), tensor(1.00000e-03 *\n",
       "         2.0015), tensor(1.00000e-03 *\n",
       "         2.4286), tensor(1.00000e-03 *\n",
       "         2.1408), tensor(1.00000e-03 *\n",
       "         2.3671), tensor(1.00000e-03 *\n",
       "         2.1798), tensor(1.00000e-03 *\n",
       "         2.2439), tensor(1.00000e-03 *\n",
       "         2.2178), tensor(1.00000e-03 *\n",
       "         2.1206), tensor(1.00000e-03 *\n",
       "         2.1881), tensor(1.00000e-03 *\n",
       "         2.0413), tensor(1.00000e-03 *\n",
       "         2.0939), tensor(1.00000e-03 *\n",
       "         1.9801), tensor(1.00000e-03 *\n",
       "         2.0375), tensor(1.00000e-03 *\n",
       "         1.9364), tensor(1.00000e-03 *\n",
       "         2.1082), tensor(1.00000e-03 *\n",
       "         2.0685), tensor(1.00000e-03 *\n",
       "         2.1037), tensor(1.00000e-03 *\n",
       "         1.9046), tensor(1.00000e-03 *\n",
       "         1.9553), tensor(1.00000e-03 *\n",
       "         1.8769), tensor(1.00000e-03 *\n",
       "         1.7566), tensor(1.00000e-03 *\n",
       "         1.8502), tensor(1.00000e-03 *\n",
       "         1.9427), tensor(1.00000e-03 *\n",
       "         1.9106), tensor(1.00000e-03 *\n",
       "         1.8943), tensor(1.00000e-03 *\n",
       "         1.8298), tensor(1.00000e-03 *\n",
       "         1.8841), tensor(1.00000e-03 *\n",
       "         1.8038), tensor(1.00000e-03 *\n",
       "         1.6871), tensor(1.00000e-03 *\n",
       "         1.7457), tensor(1.00000e-03 *\n",
       "         2.0377), tensor(1.00000e-03 *\n",
       "         1.7580), tensor(1.00000e-03 *\n",
       "         1.8963), tensor(1.00000e-03 *\n",
       "         1.7410), tensor(1.00000e-03 *\n",
       "         1.7783), tensor(1.00000e-03 *\n",
       "         1.7064), tensor(1.00000e-03 *\n",
       "         1.7887), tensor(1.00000e-03 *\n",
       "         1.7446), tensor(1.00000e-03 *\n",
       "         1.7371), tensor(1.00000e-03 *\n",
       "         1.5694), tensor(1.00000e-03 *\n",
       "         1.7422), tensor(1.00000e-03 *\n",
       "         1.8125), tensor(1.00000e-03 *\n",
       "         1.5028), tensor(1.00000e-03 *\n",
       "         1.6876), tensor(1.00000e-03 *\n",
       "         1.7368), tensor(1.00000e-03 *\n",
       "         1.7553), tensor(1.00000e-03 *\n",
       "         1.5492), tensor(1.00000e-03 *\n",
       "         1.8060), tensor(1.00000e-03 *\n",
       "         1.6959), tensor(1.00000e-03 *\n",
       "         1.6772), tensor(1.00000e-03 *\n",
       "         1.6428), tensor(1.00000e-03 *\n",
       "         1.6967), tensor(1.00000e-03 *\n",
       "         1.6549), tensor(1.00000e-03 *\n",
       "         1.6030), tensor(1.00000e-03 *\n",
       "         1.6291), tensor(1.00000e-03 *\n",
       "         1.6232), tensor(1.00000e-03 *\n",
       "         1.5872), tensor(1.00000e-03 *\n",
       "         1.6687), tensor(1.00000e-03 *\n",
       "         1.5188), tensor(1.00000e-03 *\n",
       "         1.5511), tensor(1.00000e-03 *\n",
       "         1.4002), tensor(1.00000e-03 *\n",
       "         1.5691), tensor(1.00000e-03 *\n",
       "         1.5366), tensor(1.00000e-03 *\n",
       "         1.5539), tensor(1.00000e-03 *\n",
       "         1.3535), tensor(1.00000e-03 *\n",
       "         1.5222), tensor(1.00000e-03 *\n",
       "         1.5761), tensor(1.00000e-03 *\n",
       "         1.5867), tensor(1.00000e-03 *\n",
       "         1.5098), tensor(1.00000e-03 *\n",
       "         1.5473), tensor(1.00000e-03 *\n",
       "         1.3307), tensor(1.00000e-03 *\n",
       "         1.5134), tensor(1.00000e-03 *\n",
       "         1.6079), tensor(1.00000e-03 *\n",
       "         1.6450), tensor(1.00000e-03 *\n",
       "         1.2731), tensor(1.00000e-03 *\n",
       "         1.4047), tensor(1.00000e-03 *\n",
       "         1.4187), tensor(1.00000e-03 *\n",
       "         1.3352), tensor(1.00000e-03 *\n",
       "         1.4734), tensor(1.00000e-03 *\n",
       "         1.5510), tensor(1.00000e-03 *\n",
       "         1.4307), tensor(1.00000e-03 *\n",
       "         1.2686), tensor(1.00000e-03 *\n",
       "         1.3636), tensor(1.00000e-03 *\n",
       "         1.4037), tensor(1.00000e-03 *\n",
       "         1.4477), tensor(1.00000e-03 *\n",
       "         1.3541), tensor(1.00000e-03 *\n",
       "         1.3793), tensor(1.00000e-03 *\n",
       "         1.4338), tensor(1.00000e-03 *\n",
       "         1.5244), tensor(1.00000e-03 *\n",
       "         1.4036), tensor(1.00000e-03 *\n",
       "         1.2810), tensor(1.00000e-03 *\n",
       "         1.2752), tensor(1.00000e-03 *\n",
       "         1.2645), tensor(1.00000e-03 *\n",
       "         1.3950), tensor(1.00000e-03 *\n",
       "         1.2890), tensor(1.00000e-03 *\n",
       "         1.3191), tensor(1.00000e-03 *\n",
       "         1.4159), tensor(1.00000e-03 *\n",
       "         1.2251), tensor(1.00000e-03 *\n",
       "         1.2088), tensor(1.00000e-03 *\n",
       "         1.3404), tensor(1.00000e-03 *\n",
       "         1.3186), tensor(1.00000e-03 *\n",
       "         1.3000), tensor(1.00000e-03 *\n",
       "         1.2142), tensor(1.00000e-03 *\n",
       "         1.3036), tensor(1.00000e-03 *\n",
       "         1.2851), tensor(1.00000e-03 *\n",
       "         1.2676), tensor(1.00000e-03 *\n",
       "         1.2855), tensor(1.00000e-03 *\n",
       "         1.2222), tensor(1.00000e-03 *\n",
       "         1.2348), tensor(1.00000e-03 *\n",
       "         1.2406), tensor(1.00000e-03 *\n",
       "         1.3084), tensor(1.00000e-03 *\n",
       "         1.3389)], 'lexvec': [tensor(1.00000e-03 *\n",
       "         4.6629), tensor(1.00000e-03 *\n",
       "         4.5237), tensor(1.00000e-03 *\n",
       "         4.1669), tensor(1.00000e-03 *\n",
       "         3.7997), tensor(1.00000e-03 *\n",
       "         3.6481), tensor(1.00000e-03 *\n",
       "         3.5432), tensor(1.00000e-03 *\n",
       "         3.4085), tensor(1.00000e-03 *\n",
       "         3.2553), tensor(1.00000e-03 *\n",
       "         3.3590), tensor(1.00000e-03 *\n",
       "         3.0881), tensor(1.00000e-03 *\n",
       "         3.0370), tensor(1.00000e-03 *\n",
       "         3.0971), tensor(1.00000e-03 *\n",
       "         3.0891), tensor(1.00000e-03 *\n",
       "         3.0586), tensor(1.00000e-03 *\n",
       "         2.9567), tensor(1.00000e-03 *\n",
       "         2.8959), tensor(1.00000e-03 *\n",
       "         2.9514), tensor(1.00000e-03 *\n",
       "         2.6964), tensor(1.00000e-03 *\n",
       "         2.6802), tensor(1.00000e-03 *\n",
       "         2.7260), tensor(1.00000e-03 *\n",
       "         2.7794), tensor(1.00000e-03 *\n",
       "         2.7248), tensor(1.00000e-03 *\n",
       "         2.5940), tensor(1.00000e-03 *\n",
       "         2.3265), tensor(1.00000e-03 *\n",
       "         2.6811), tensor(1.00000e-03 *\n",
       "         2.4457), tensor(1.00000e-03 *\n",
       "         2.3823), tensor(1.00000e-03 *\n",
       "         2.2953), tensor(1.00000e-03 *\n",
       "         2.4796), tensor(1.00000e-03 *\n",
       "         2.3738), tensor(1.00000e-03 *\n",
       "         2.3475), tensor(1.00000e-03 *\n",
       "         2.4898), tensor(1.00000e-03 *\n",
       "         2.4425), tensor(1.00000e-03 *\n",
       "         2.2958), tensor(1.00000e-03 *\n",
       "         2.2626), tensor(1.00000e-03 *\n",
       "         2.3604), tensor(1.00000e-03 *\n",
       "         2.2485), tensor(1.00000e-03 *\n",
       "         2.1456), tensor(1.00000e-03 *\n",
       "         2.3605), tensor(1.00000e-03 *\n",
       "         1.9726), tensor(1.00000e-03 *\n",
       "         2.4109), tensor(1.00000e-03 *\n",
       "         2.1029), tensor(1.00000e-03 *\n",
       "         2.3751), tensor(1.00000e-03 *\n",
       "         2.1819), tensor(1.00000e-03 *\n",
       "         2.3182), tensor(1.00000e-03 *\n",
       "         2.2370), tensor(1.00000e-03 *\n",
       "         2.1296), tensor(1.00000e-03 *\n",
       "         2.1987), tensor(1.00000e-03 *\n",
       "         1.9943), tensor(1.00000e-03 *\n",
       "         2.0629), tensor(1.00000e-03 *\n",
       "         2.0013), tensor(1.00000e-03 *\n",
       "         2.0455), tensor(1.00000e-03 *\n",
       "         1.9386), tensor(1.00000e-03 *\n",
       "         2.1084), tensor(1.00000e-03 *\n",
       "         2.0622), tensor(1.00000e-03 *\n",
       "         2.0903), tensor(1.00000e-03 *\n",
       "         1.9039), tensor(1.00000e-03 *\n",
       "         1.9648), tensor(1.00000e-03 *\n",
       "         1.8699), tensor(1.00000e-03 *\n",
       "         1.7147), tensor(1.00000e-03 *\n",
       "         1.8099), tensor(1.00000e-03 *\n",
       "         1.9516), tensor(1.00000e-03 *\n",
       "         1.8847), tensor(1.00000e-03 *\n",
       "         1.8817), tensor(1.00000e-03 *\n",
       "         1.7960), tensor(1.00000e-03 *\n",
       "         1.8666), tensor(1.00000e-03 *\n",
       "         1.8037), tensor(1.00000e-03 *\n",
       "         1.6718), tensor(1.00000e-03 *\n",
       "         1.7435), tensor(1.00000e-03 *\n",
       "         2.0336), tensor(1.00000e-03 *\n",
       "         1.7478), tensor(1.00000e-03 *\n",
       "         1.8718), tensor(1.00000e-03 *\n",
       "         1.7768), tensor(1.00000e-03 *\n",
       "         1.8007), tensor(1.00000e-03 *\n",
       "         1.6744), tensor(1.00000e-03 *\n",
       "         1.7695), tensor(1.00000e-03 *\n",
       "         1.7256), tensor(1.00000e-03 *\n",
       "         1.7309), tensor(1.00000e-03 *\n",
       "         1.5753), tensor(1.00000e-03 *\n",
       "         1.7530), tensor(1.00000e-03 *\n",
       "         1.7805), tensor(1.00000e-03 *\n",
       "         1.4759), tensor(1.00000e-03 *\n",
       "         1.6784), tensor(1.00000e-03 *\n",
       "         1.7039), tensor(1.00000e-03 *\n",
       "         1.7713), tensor(1.00000e-03 *\n",
       "         1.5189), tensor(1.00000e-03 *\n",
       "         1.8154), tensor(1.00000e-03 *\n",
       "         1.6984), tensor(1.00000e-03 *\n",
       "         1.6650), tensor(1.00000e-03 *\n",
       "         1.6365), tensor(1.00000e-03 *\n",
       "         1.6961), tensor(1.00000e-03 *\n",
       "         1.6683), tensor(1.00000e-03 *\n",
       "         1.6267), tensor(1.00000e-03 *\n",
       "         1.6523), tensor(1.00000e-03 *\n",
       "         1.6172), tensor(1.00000e-03 *\n",
       "         1.5529), tensor(1.00000e-03 *\n",
       "         1.6223), tensor(1.00000e-03 *\n",
       "         1.4992), tensor(1.00000e-03 *\n",
       "         1.5493), tensor(1.00000e-03 *\n",
       "         1.3812), tensor(1.00000e-03 *\n",
       "         1.5142), tensor(1.00000e-03 *\n",
       "         1.5142), tensor(1.00000e-03 *\n",
       "         1.5555), tensor(1.00000e-03 *\n",
       "         1.3162), tensor(1.00000e-03 *\n",
       "         1.5408), tensor(1.00000e-03 *\n",
       "         1.5871), tensor(1.00000e-03 *\n",
       "         1.5561), tensor(1.00000e-03 *\n",
       "         1.5236), tensor(1.00000e-03 *\n",
       "         1.5382), tensor(1.00000e-03 *\n",
       "         1.3103), tensor(1.00000e-03 *\n",
       "         1.5093), tensor(1.00000e-03 *\n",
       "         1.6076), tensor(1.00000e-03 *\n",
       "         1.6287), tensor(1.00000e-03 *\n",
       "         1.2660), tensor(1.00000e-03 *\n",
       "         1.3958), tensor(1.00000e-03 *\n",
       "         1.3994), tensor(1.00000e-03 *\n",
       "         1.3387), tensor(1.00000e-03 *\n",
       "         1.4556), tensor(1.00000e-03 *\n",
       "         1.5537), tensor(1.00000e-03 *\n",
       "         1.4151), tensor(1.00000e-03 *\n",
       "         1.2710), tensor(1.00000e-03 *\n",
       "         1.3689), tensor(1.00000e-03 *\n",
       "         1.4249), tensor(1.00000e-03 *\n",
       "         1.4711), tensor(1.00000e-03 *\n",
       "         1.3580), tensor(1.00000e-03 *\n",
       "         1.3602), tensor(1.00000e-03 *\n",
       "         1.4388), tensor(1.00000e-03 *\n",
       "         1.5317), tensor(1.00000e-03 *\n",
       "         1.3776), tensor(1.00000e-03 *\n",
       "         1.2588), tensor(1.00000e-03 *\n",
       "         1.2417), tensor(1.00000e-03 *\n",
       "         1.2495), tensor(1.00000e-03 *\n",
       "         1.3935), tensor(1.00000e-03 *\n",
       "         1.3066), tensor(1.00000e-03 *\n",
       "         1.3269), tensor(1.00000e-03 *\n",
       "         1.4000), tensor(1.00000e-03 *\n",
       "         1.2333), tensor(1.00000e-03 *\n",
       "         1.2007), tensor(1.00000e-03 *\n",
       "         1.3249), tensor(1.00000e-03 *\n",
       "         1.3203), tensor(1.00000e-03 *\n",
       "         1.2890), tensor(1.00000e-03 *\n",
       "         1.2261), tensor(1.00000e-03 *\n",
       "         1.2978), tensor(1.00000e-03 *\n",
       "         1.3038), tensor(1.00000e-03 *\n",
       "         1.2311), tensor(1.00000e-03 *\n",
       "         1.2565), tensor(1.00000e-03 *\n",
       "         1.2313), tensor(1.00000e-03 *\n",
       "         1.2169), tensor(1.00000e-03 *\n",
       "         1.2095), tensor(1.00000e-03 *\n",
       "         1.2921), tensor(1.00000e-03 *\n",
       "         1.3179)], 'skipgram': [tensor(1.00000e-03 *\n",
       "         4.5647), tensor(1.00000e-03 *\n",
       "         4.2826), tensor(1.00000e-03 *\n",
       "         4.0516), tensor(1.00000e-03 *\n",
       "         3.7432), tensor(1.00000e-03 *\n",
       "         3.5411), tensor(1.00000e-03 *\n",
       "         3.4287), tensor(1.00000e-03 *\n",
       "         3.3140), tensor(1.00000e-03 *\n",
       "         3.1911), tensor(1.00000e-03 *\n",
       "         3.3147), tensor(1.00000e-03 *\n",
       "         3.0768), tensor(1.00000e-03 *\n",
       "         3.0083), tensor(1.00000e-03 *\n",
       "         3.0596), tensor(1.00000e-03 *\n",
       "         3.0645), tensor(1.00000e-03 *\n",
       "         3.0494), tensor(1.00000e-03 *\n",
       "         2.9673), tensor(1.00000e-03 *\n",
       "         2.8731), tensor(1.00000e-03 *\n",
       "         2.9441), tensor(1.00000e-03 *\n",
       "         2.6339), tensor(1.00000e-03 *\n",
       "         2.6927), tensor(1.00000e-03 *\n",
       "         2.7264), tensor(1.00000e-03 *\n",
       "         2.7867), tensor(1.00000e-03 *\n",
       "         2.6826), tensor(1.00000e-03 *\n",
       "         2.5507), tensor(1.00000e-03 *\n",
       "         2.3566), tensor(1.00000e-03 *\n",
       "         2.6951), tensor(1.00000e-03 *\n",
       "         2.4405), tensor(1.00000e-03 *\n",
       "         2.3745), tensor(1.00000e-03 *\n",
       "         2.3064), tensor(1.00000e-03 *\n",
       "         2.4441), tensor(1.00000e-03 *\n",
       "         2.3168), tensor(1.00000e-03 *\n",
       "         2.3663), tensor(1.00000e-03 *\n",
       "         2.4681), tensor(1.00000e-03 *\n",
       "         2.4461), tensor(1.00000e-03 *\n",
       "         2.2731), tensor(1.00000e-03 *\n",
       "         2.2565), tensor(1.00000e-03 *\n",
       "         2.3696), tensor(1.00000e-03 *\n",
       "         2.2635), tensor(1.00000e-03 *\n",
       "         2.1530), tensor(1.00000e-03 *\n",
       "         2.3792), tensor(1.00000e-03 *\n",
       "         1.9982), tensor(1.00000e-03 *\n",
       "         2.4153), tensor(1.00000e-03 *\n",
       "         2.1319), tensor(1.00000e-03 *\n",
       "         2.3498), tensor(1.00000e-03 *\n",
       "         2.1603), tensor(1.00000e-03 *\n",
       "         2.2766), tensor(1.00000e-03 *\n",
       "         2.1976), tensor(1.00000e-03 *\n",
       "         2.1400), tensor(1.00000e-03 *\n",
       "         2.2022), tensor(1.00000e-03 *\n",
       "         2.0222), tensor(1.00000e-03 *\n",
       "         2.0730), tensor(1.00000e-03 *\n",
       "         2.0073), tensor(1.00000e-03 *\n",
       "         2.0326), tensor(1.00000e-03 *\n",
       "         1.9521), tensor(1.00000e-03 *\n",
       "         2.0942), tensor(1.00000e-03 *\n",
       "         2.0691), tensor(1.00000e-03 *\n",
       "         2.1078), tensor(1.00000e-03 *\n",
       "         1.9307), tensor(1.00000e-03 *\n",
       "         1.9339), tensor(1.00000e-03 *\n",
       "         1.8521), tensor(1.00000e-03 *\n",
       "         1.7431), tensor(1.00000e-03 *\n",
       "         1.8456), tensor(1.00000e-03 *\n",
       "         1.9324), tensor(1.00000e-03 *\n",
       "         1.9169), tensor(1.00000e-03 *\n",
       "         1.8952), tensor(1.00000e-03 *\n",
       "         1.7855), tensor(1.00000e-03 *\n",
       "         1.9153), tensor(1.00000e-03 *\n",
       "         1.8383), tensor(1.00000e-03 *\n",
       "         1.6676), tensor(1.00000e-03 *\n",
       "         1.7445), tensor(1.00000e-03 *\n",
       "         2.0300), tensor(1.00000e-03 *\n",
       "         1.7777), tensor(1.00000e-03 *\n",
       "         1.8882), tensor(1.00000e-03 *\n",
       "         1.7465), tensor(1.00000e-03 *\n",
       "         1.7955), tensor(1.00000e-03 *\n",
       "         1.6929), tensor(1.00000e-03 *\n",
       "         1.7762), tensor(1.00000e-03 *\n",
       "         1.7572), tensor(1.00000e-03 *\n",
       "         1.7123), tensor(1.00000e-03 *\n",
       "         1.5877), tensor(1.00000e-03 *\n",
       "         1.7813), tensor(1.00000e-03 *\n",
       "         1.8049), tensor(1.00000e-03 *\n",
       "         1.4776), tensor(1.00000e-03 *\n",
       "         1.7030), tensor(1.00000e-03 *\n",
       "         1.7208), tensor(1.00000e-03 *\n",
       "         1.7417), tensor(1.00000e-03 *\n",
       "         1.5315), tensor(1.00000e-03 *\n",
       "         1.8166), tensor(1.00000e-03 *\n",
       "         1.7069), tensor(1.00000e-03 *\n",
       "         1.7014), tensor(1.00000e-03 *\n",
       "         1.6248), tensor(1.00000e-03 *\n",
       "         1.7056), tensor(1.00000e-03 *\n",
       "         1.6921), tensor(1.00000e-03 *\n",
       "         1.6548), tensor(1.00000e-03 *\n",
       "         1.6487), tensor(1.00000e-03 *\n",
       "         1.6043), tensor(1.00000e-03 *\n",
       "         1.5425), tensor(1.00000e-03 *\n",
       "         1.6357), tensor(1.00000e-03 *\n",
       "         1.5062), tensor(1.00000e-03 *\n",
       "         1.5638), tensor(1.00000e-03 *\n",
       "         1.3973), tensor(1.00000e-03 *\n",
       "         1.5501), tensor(1.00000e-03 *\n",
       "         1.5600), tensor(1.00000e-03 *\n",
       "         1.5713), tensor(1.00000e-03 *\n",
       "         1.3380), tensor(1.00000e-03 *\n",
       "         1.5097), tensor(1.00000e-03 *\n",
       "         1.5736), tensor(1.00000e-03 *\n",
       "         1.5976), tensor(1.00000e-03 *\n",
       "         1.5197), tensor(1.00000e-03 *\n",
       "         1.5187), tensor(1.00000e-03 *\n",
       "         1.3122), tensor(1.00000e-03 *\n",
       "         1.5043), tensor(1.00000e-03 *\n",
       "         1.6129), tensor(1.00000e-03 *\n",
       "         1.6393), tensor(1.00000e-03 *\n",
       "         1.2914), tensor(1.00000e-03 *\n",
       "         1.4061), tensor(1.00000e-03 *\n",
       "         1.3950), tensor(1.00000e-03 *\n",
       "         1.3453), tensor(1.00000e-03 *\n",
       "         1.4933), tensor(1.00000e-03 *\n",
       "         1.5584), tensor(1.00000e-03 *\n",
       "         1.4303), tensor(1.00000e-03 *\n",
       "         1.2970), tensor(1.00000e-03 *\n",
       "         1.3771), tensor(1.00000e-03 *\n",
       "         1.4283), tensor(1.00000e-03 *\n",
       "         1.4860), tensor(1.00000e-03 *\n",
       "         1.3550), tensor(1.00000e-03 *\n",
       "         1.3982), tensor(1.00000e-03 *\n",
       "         1.4283), tensor(1.00000e-03 *\n",
       "         1.5387), tensor(1.00000e-03 *\n",
       "         1.3903), tensor(1.00000e-03 *\n",
       "         1.2571), tensor(1.00000e-03 *\n",
       "         1.2998), tensor(1.00000e-03 *\n",
       "         1.2251), tensor(1.00000e-03 *\n",
       "         1.3864), tensor(1.00000e-03 *\n",
       "         1.3301), tensor(1.00000e-03 *\n",
       "         1.3308), tensor(1.00000e-03 *\n",
       "         1.4169), tensor(1.00000e-03 *\n",
       "         1.2446), tensor(1.00000e-03 *\n",
       "         1.2226), tensor(1.00000e-03 *\n",
       "         1.3366), tensor(1.00000e-03 *\n",
       "         1.3520), tensor(1.00000e-03 *\n",
       "         1.3118), tensor(1.00000e-03 *\n",
       "         1.2108), tensor(1.00000e-03 *\n",
       "         1.2848), tensor(1.00000e-03 *\n",
       "         1.2861), tensor(1.00000e-03 *\n",
       "         1.2273), tensor(1.00000e-03 *\n",
       "         1.2475), tensor(1.00000e-03 *\n",
       "         1.2033), tensor(1.00000e-03 *\n",
       "         1.2377), tensor(1.00000e-03 *\n",
       "         1.2279), tensor(1.00000e-03 *\n",
       "         1.3076), tensor(1.00000e-03 *\n",
       "         1.3285)]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_embedding_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def get_relational_vectors(embedding_dictionary, target):\n",
    "    source_vecs = []\n",
    "    for (name, vecs) in embedding_dictionary.items():\n",
    "        print(name.lower())\n",
    "        if target.lower() not in name.lower():\n",
    "            l = list(vecs.values())\n",
    "            source_vecs.append(np.vstack(l))\n",
    "        else:\n",
    "            target_vec = np.vstack(list(vecs.values()))\n",
    "    \n",
    "    source_vecs = np.hstack(source_vecs)\n",
    "    \n",
    "    print(\"SOURCE\")\n",
    "    print(source_vecs.shape)\n",
    "    print(\"TARGET\")\n",
    "    print(target_vec.shape)\n",
    "    \n",
    "    # remember to cut end off for target\n",
    "    dims = (source_vecs.shape[1], target_vec.shape[1])\n",
    "    vectors = np.hstack([source_vecs, target_vec])\n",
    "    vec_batches = torch.utils.data.DataLoader(vectors, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "    return (vec_batches, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "fasttext\n",
      "skipgram\n",
      "hdc\n",
      "lexvec\n",
      "hpca\n",
      "SOURCE\n",
      "(4819, 1400)\n",
      "TARGET\n",
      "(4819, 300)\n"
     ]
    }
   ],
   "source": [
    "x, dims = get_relational_vectors(embedding_dictionary, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(x):\n",
    "    x1, x2  = Variable(data[:,:dims[1]]), Variable(data[:,:-dims[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 300])\n",
      "torch.Size([19, 1400])\n"
     ]
    }
   ],
   "source": [
    "print(x1.size())\n",
    "print(x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    " Simple example showing evaluating embedding on similarity datasets\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def size_splits(tensor, split_sizes, dim=0):\n",
    "    \"\"\"Splits the tensor according to chunks of split_sizes.\n",
    "\n",
    "    Arguments:\n",
    "        tensor (Tensor): tensor to split.\n",
    "        split_sizes (list(int)): sizes of chunks\n",
    "        dim (int): dimension along which to split the tensor.\n",
    "    \"\"\"\n",
    "    if dim < 0:\n",
    "        dim += tensor.dim()\n",
    "\n",
    "    dim_size = tensor.size(dim)\n",
    "\n",
    "    print(tensor.size())\n",
    "\n",
    "    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n",
    "        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n",
    "\n",
    "    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n",
    "\n",
    "    return tuple(tensor.narrow(int(dim), int(start), int(length))\n",
    "                 for start, length in zip(splits, split_sizes))\n",
    "\n",
    "def batch_vecs(vecs, x, y):\n",
    "    x1,x2 = get_vector_pairs(vecs, x, y)\n",
    "    x1, x2 = torch.FloatTensor(x1), torch.FloatTensor(x2)\n",
    "    x = torch.utils.data.DataLoader((x1,x2), batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "    return(x)\n",
    "\n",
    "\n",
    "def load_vectors(source_embedding='glove',target_embedding='sg', tasks=None):\n",
    "    task_vectors = {\"MEN\":{},\"WS353\": {},\"SIMLEX999\": {}}\n",
    "    x1,x2 = load_vector(vecs, x, y)\n",
    "    x1, x2 = torch.FloatTensor(x1), torch.FloatTensor(x2)\n",
    "    x = torch.utils.data.DataLoader((x1,x2), batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "    return(x)\n",
    "\n",
    "def get_vectors(source_embedding='glove',\n",
    "                target_embedding='sg', tasks=None):\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "    # Fetch GloVe embedding (warning: it might take few minutes)\n",
    "\n",
    "\n",
    "    w_glove = fetch_GloVe(corpus=\"wiki-6B\", dim=300)\n",
    "    w_skipgram = fetch_SG_GoogleNews()\n",
    "\n",
    "    # Define tasks\n",
    "    tasks = {\n",
    "        \"MEN\": fetch_MEN(),\n",
    "        \"WS353\": fetch_WS353(),\n",
    "        \"SIMLEX999\": fetch_SimLex999()\n",
    "    }\n",
    "\n",
    "    task_vectors = {\"MEN\":{},\n",
    "                    \"WS353\": {},\n",
    "                    \"SIMLEX999\": {}\n",
    "    }\n",
    "    # Calculate results using helper function\n",
    "\n",
    "    for name, data in iteritems(tasks):\n",
    "        vecs = {source_embedding:w_glove,target_embedding:w_skipgram}\n",
    "        x_source, x_target = batch_vecs(vecs, data.X, data.y)\n",
    "        task_vectors[name] = (x_source, x_target)\n",
    "    return (task_vectors)\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    task = args.dataset.upper()\n",
    "    args.word_dims = args.vector_pairs[0][0].size(1)\n",
    "    args.fixed_dim = int(args.word_dims/3.0)\n",
    "\n",
    "    try:\n",
    "        args.vector_pairs = args.vector_pairs[task]\n",
    "    except:\n",
    "        ValueError(\"You must pick the right dataset for vector pair\")\n",
    "\n",
    "    model = Autoencoder(args)\n",
    "\n",
    "    if model:\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.0005)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in args.epochs:\n",
    "        for i, (x1,x2) in enumerate(args.vector_pairs):\n",
    "            x1, x2 = Variable(x1), Variable(x2)\n",
    "            if args.cuda:\n",
    "                x1, x2 = x1.cuda(), x2.cuda()\n",
    "\n",
    "            y = model(x1)\n",
    "            loss = criterion(y, x2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ===================log========================\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, epoch, loss.data[0]))\n",
    "\n",
    "\n",
    "ds = ['simlex', 'rareword', 'sim353', 'men', 'simverb', 'sn', 'bless']\n",
    "embeddings = ['cbow', 'glove', 'word2vec', 'hlbl', 'pmi']\n",
    "save = False\n",
    "\n",
    "# nhid set to 100 here to be equal to emsize given they should be equal for tied weights\n",
    "# bptt decides the number of time steps (look at get_batch in gru_helper)\n",
    "\n",
    "parse = argparse.ArgumentParser()\n",
    "\n",
    "# fixed dim is the set size of the hidden dimensions of the autoencoders\n",
    "# so to average over the same dimensions\n",
    "vec_pair = get_vectors()\n",
    "parse.add_argument(\"--vector_pairs\",default=vec_pair)\n",
    "parse.add_argument(\"--dataset\",default='men')\n",
    "\n",
    "parse.add_argument(\"--fixed_dim\", default=100, type=int)\n",
    "parse.add_argument(\"--pretrain\", default=True, type=int)\n",
    "parse.add_argument(\"--data\", default=ds)\n",
    "parse.add_argument(\"--model\", default='ae', type=str)\n",
    "parse.add_argument(\"--attention\", default=False, type=bool)\n",
    "parse.add_argument(\"--attention_width\", default=5, type=int)\n",
    "parse.add_argument(\"--embeddings\", default=embeddings)\n",
    "parse.add_argument(\"--clip\", default=0.0001, type=float)\n",
    "parse.add_argument(\"--lr\", default=0.001, type=float)\n",
    "parse.add_argument(\"--num_layers\", default=True, type=int)\n",
    "parse.add_argument(\"--cuda\", default=False, type=bool)\n",
    "parse.add_argument(\"--epochs\", default=100, type=int)\n",
    "parse.add_argument(\"--optimizer\", default='adam', type=str)\n",
    "parse.add_argument(\"--train_batch_size\", default=20, type=int)\n",
    "parse.add_argument(\"--eval_batch_size\", default=20, type=int)\n",
    "parse.add_argument(\"--log-interval\", default=10, type=int)\n",
    "\n",
    "args = parse.parse_args()\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args['seed'])\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
